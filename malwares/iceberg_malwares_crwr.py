from iceberglist import *

baseurl = "http://story.malwares.com"
targeturl = "http://story.malwares.com/category/%EB%B3%B4%EC%95%88%20%EC%A0%95%EB%B3%B4/%EC%95%85%EC%84%B1%EC%BD%94%EB%93%9C%20%EB%B6%84%EC%84%9D%EB%B3%B4%EA%B3%A0%EC%84%9C" # 분석보고서 
pageurl =   "http://story.malwares.com/category/%EB%B3%B4%EC%95%88%20%EC%A0%95%EB%B3%B4/%EC%95%85%EC%84%B1%EC%BD%94%EB%93%9C%20%EB%B6%84%EC%84%9D%EB%B3%B4%EA%B3%A0%EC%84%9C?page={}"

targeturl2 ="http://story.malwares.com/category/%EB%B3%B4%EC%95%88%20%EC%A0%95%EB%B3%B4/%EC%95%85%EC%84%B1%EC%BD%94%EB%93%9C%20%EA%B4%80%EB%A0%A8%20%EC%A0%95%EB%B3%B4" # 악성코드
pageurl2 = "http://story.malwares.com/category/%EB%B3%B4%EC%95%88%20%EC%A0%95%EB%B3%B4/%EC%95%85%EC%84%B1%EC%BD%94%EB%93%9C%20%EA%B4%80%EB%A0%A8%20%EC%A0%95%EB%B3%B4?page={}" # 악성코드

##############################################################################################################################################################################################
def malwares_crwrscript(collection,url):

	try:
		html = requesturl(url).read().decode()
		soup = BeautifulSoup(html,'html.parser')
		onesoup = soup.select('#tt-body-page > div.jb-page.jb-youtube-auto > div.jb-background.jb-background-main > div > div > div.jb-column.jb-column-content > div.jb-cell.jb-cell-content.jb-cell-content-article > article')[0]
		title = (onesoup.select('div.jb-content-title.jb-content-title-article > h2 > a')[0].text)
		date = (onesoup.select('div.jb-article-information > ul > li > span')[-1].text.split()[0])
		date_split = date.split('.')
		filter_date = date_split[1]+'.'+date_split[2]+'.'+date_split[0]
		rawhtml = soup.select('section > div > div > div')[0] # .get_text('\n').strip())
		testdict ={}
		testdict.update(returnimg(rawhtml,url,baseurl))
		taglist = [x.text for x in soup.select('div.jb-sidebar-content.jb-sidebar-content-tags > ul > li')]
		testdict['tag'] = taglist
		testdict['date'] = filter_date
		testdict['title'] = title
		testdict['text'] = BeautifulSoup(testdict['html'],'html.parser').get_text('\n').strip()
		# if(dupcheck(collection,url,testdict['text'])):
			
		# 	return None
		print(testdict['title'],url)
		return testdict
	except Exception as e:
		print(e,url)

def search(soup):
	link_list = ([x.find("a")['href'] if baseurl in x.find("a")['href'] else baseurl+x.find("a")['href'] for x in soup.findAll("h3" ,{"class":"jb-index-title"})])
	#print(link_list)
	print('\n\n')
	return link_list


def urllist_search(collection):
	html = requesturl(targeturl).read().decode()
	soup = BeautifulSoup(html,'html.parser')

	lastpage = (int(soup.select('div.jb-cell.jb-cell-pagination li > a > span')[-1].text)) # 1 페이지 작업 
	print(lastpage)

	for url in search(soup): # 서칭
		mongohelp(collection,malwares_crwrscript,url)
	for i in range(2,lastpage+1): # 2,3, 페이지 작업
		html = requesturl(pageurl.format(i)).read().decode()
		soup = BeautifulSoup(html,'html.parser')
		for url in search(soup): # 서칭
			mongohelp(collection,malwares_crwrscript,url)

	html2 = requesturl(targeturl2).read().decode()
	soup2 = BeautifulSoup(html2,'html.parser')

	lastpage = (int(soup2.select('div.jb-cell.jb-cell-pagination li > a > span')[-1].text)) # 1 페이지 작업 
	print(lastpage)
	for url in search(soup2): # 서칭
		
		mongohelp(collection,malwares_crwrscript,url)
	for i in range(2,lastpage+1): # 2,3, 페이지 작업
		html2 = requesturl(pageurl2.format(i)).read().decode()
		soup2 = BeautifulSoup(html2,'html.parser')
		for url in search(soup2): # 서칭
			
			mongohelp(collection,malwares_crwrscript,url)


if __name__ == '__main__':
	
	client = MongoClient('127.0.0.1',27017)
	my_db = client.iceberg
	my_collection = my_db.malwares
	urllist_search(my_collection)
	client.close()